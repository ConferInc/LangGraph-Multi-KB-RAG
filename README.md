#  Multi-Source RAG Orchestrator

Visit: http://xc0cssg0ook40ckwgk4kocko.144.126.158.171.sslip.io/
![System Architecture](graph.png)

## ğŸ“‹ Overview

The **Intelligent Multi-Source RAG Orchestrator** is a sophisticated AI system that intelligently routes user queries to specific knowledge bases using semantic understanding. Built with LangGraph, it implements a Router-Retriever-Generator architecture that eliminates hallucinations and ensures precise, context-aware responses.

### ğŸ¯ Key Features

- **ğŸ§  Intelligent Query Routing**: Automatically classifies queries into MoXi (Mortgage/Loans), Confer (Platform Features), or General conversation
- **ğŸ“š Multi-Source RAG**: Retrieves from distinct Qdrant vector databases based on query intent
- **ğŸ”’ Safety-First Design**: Built-in content filtering and harmful query detection
- **âš¡ Stateful Orchestration**: LangGraph-powered workflow with conditional execution paths
- **ğŸ¨ Modern UI**: Beautiful Next.js frontend with real-time chat interface
- **ğŸ³ Production Ready**: Dockerized services optimized for Coolify deployment

---

## ğŸ—ï¸ Architecture

The system operates as a Directed Acyclic Graph (DAG) with the following workflow:

```
User Query â†’ Classification â†’ Conditional Routing â†’ Retrieval â†’ Response Generation
                    â†“              â†“         â†“
                 [moxi]        [confer]  [general]
                    â†“              â†“         â†“
              MoXi DB        Confer DB    (Skip)
                    â†“              â†“         â†“
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ LLM Response
```

### Components

1. **Classification Node**: Semantic intent analysis with deterministic fallbacks
2. **Retrieval Nodes**: Parallel vector search across specialized knowledge bases
3. **Generation Node**: Context-aware response synthesis with safety guardrails

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Node.js 20+
- Docker & Docker Compose (for deployment)
- OpenAI API Key
- Qdrant Vector Database

### Local Development

#### Backend Setup

```bash
cd Backend

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your API keys

# Run the server
uvicorn server:app --reload --port 8000
```

#### Frontend Setup

```bash
cd Frontend

# Install dependencies
npm install

# Configure environment
cp env.sample .env.local
# Edit .env.local with your backend URL

# Run development server
npm run dev
```

Visit `http://localhost:3000` to access the application.

---

## ğŸ› ï¸ Technology Stack

### Backend
- **Framework**: FastAPI 0.109
- **Orchestration**: LangGraph 0.0.20
- **LLM Integration**: LangChain + OpenAI
- **Vector Database**: Qdrant 1.7.0
- **Validation**: Pydantic 2.5.3

### Frontend
- **Framework**: Next.js 16.0+
- **UI Library**: Radix UI + Tailwind CSS
- **State Management**: React Hooks
- **HTTP Client**: Axios
- **Styling**: Tailwind CSS 4.1+

---

## ğŸ”§ Configuration

### Backend Environment Variables

```env
# LLM Configuration
OPENAI_API_KEY=sk-...

# Vector Database
QDRANT_URL=https://qdrant.confersolutions.ai
QDRANT_API_KEY=...

### Frontend Environment Variables

```env
# API Configuration
NEXT_PUBLIC_API_URL=http://localhost:8000

# Environment
NODE_ENV=development
```

---

## ğŸ“Š System Workflow

### 1. Query Classification

The system first analyzes the user's intent:

```python
class RouteQuery(BaseModel):
    datasource: Literal["confer", "moxi", "general"]
```

**Deterministic Rules:**
- Contains "moxi" â†’ Route to MoXi database
- Contains "confer" â†’ Route to Confer database
- Otherwise â†’ LLM-based semantic classification

### 2. Retrieval (Conditional)

If classified as `moxi` or `confer`:
- Perform similarity search in respective Qdrant collection
- Retrieve top 4 most relevant documents
- Extract page content for context injection

### 3. Response Generation

**RAG Mode** (with retrieved documents):
- Temperature: 0.0 (factual accuracy)
- Context: Injected retrieved documents
- Principles: Accuracy, Clarity, Helpfulness

**General Mode** (no retrieval):
- Temperature: 0.5 (conversational)
- Personality: Warm, professional assistant
- Guidance: Ask for clarification on product queries

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ Backend/
â”‚   â”œâ”€â”€ app.py              # LangGraph orchestration logic
â”‚   â”œâ”€â”€ server.py           # FastAPI server
â”‚   â”œâ”€â”€ prompts.json        # Externalized prompts
â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile          # Backend container
â”‚   â””â”€â”€ .env.example        # Environment template
â”‚
â”œâ”€â”€ Frontend/
â”‚   â”œâ”€â”€ app/                # Next.js app directory
â”‚   â”œâ”€â”€ components/         # React components
â”‚   â”‚   â”œâ”€â”€ ChatWindow.tsx
â”‚   â”‚   â”œâ”€â”€ InputBox.tsx
â”‚   â”‚   â”œâ”€â”€ MessageBubble.tsx
â”‚   â”‚   â””â”€â”€ Sidebar.tsx
â”‚   â”œâ”€â”€ hooks/              # Custom React hooks
â”‚   â”œâ”€â”€ lib/                # Utility functions
â”‚   â”œâ”€â”€ public/             # Static assets
â”‚   â”œâ”€â”€ Dockerfile          # Frontend container
â”‚   â””â”€â”€ package.json        # Node dependencies
â”‚
â”œâ”€â”€ graph.png               # Architecture diagram
â”œâ”€â”€ PRD.md                  # Product requirements
â”œâ”€â”€ COOLIFY_DEPLOYMENT.md   # Deployment guide
â””â”€â”€ README.md               # This file
```


